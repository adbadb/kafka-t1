1. Create topic

docker exec -ti kafka1 /usr/bin/kafka-topics --create  --bootstrap-server kafka1:19092,kafka2:19093,kafka3:19094 --replication-factor 2 --partitions 4 --topic events2

2. Send data

docker exec -ti kafka1 /usr/bin/kafka-console-producer --bootstrap-server kafka1:19092,kafka2:19093,kafka3:19094 --topic events2  --property parse.key=true --property key.separator=,


k1,v1
k2,v2
k3,v3
k4,v4
k5,v5

k1,v1_1
k1,v1_2
k1,v1_3


3. Read the data 


docker exec -ti kafka1 /usr/bin/kafka-console-consumer --bootstrap-server kafka1:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true --property key.separator=","

No data will be shown because we do not have --from-beginning property


4. Run with specifying consumer group and printing the partition

docker exec -ti kafka1 /usr/bin/kafka-console-consumer --bootstrap-server kafka1:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true --property key.separator="," --property print.partition=true --from-beginning --group cons1


Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

5. Run in another terminal the same command 


docker exec -ti kafka1 /usr/bin/kafka-console-consumer --bootstrap-server kafka1:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key=true --property key.separator="," --property print.partition=true --from-beginning --group cons1


Now we have two consumers within one group.

6. Send more data with our producer

k6,v6
k7,v7
k8,v8
k9,v9
k10,v10

Check that messages come to different consumers
